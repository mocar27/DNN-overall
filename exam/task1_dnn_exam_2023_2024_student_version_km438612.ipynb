{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Task 1 (6p)\n",
    "Your task is to modify the custom implementation of MultiHeadAttention. This custom implementation, currently, enables each token to attent to every other token.\n",
    "\n",
    "\n",
    "Your job is to change this behavior in a specific way.\n",
    "Let $S$ be our input sequence of length $2 \\cdot k$:\n",
    "- tokens on positions $i \\lt k$ should attend to prefix of $S$ of length $k$ ($S[:k]$) - every token up to position k\n",
    "- tokens on positions $i \\ge k$ should attend to prefix of $S$  of length $i + 1$ ($S[:i + 1]$) - every previous token and itself\n",
    "\n",
    "(Note: You can assume the sequence length is always an even number)."
   ],
   "metadata": {
    "id": "VFiew9xpybfu"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import math\n",
    "import torch.nn.functional as F\n",
    "class MultiHeadAttention(torch.nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_head):\n",
    "      super().__init__()\n",
    "      self.d_model = d_model\n",
    "      self.num_heads = num_heads\n",
    "      self.d_head = d_head\n",
    "\n",
    "      self.W_Q = torch.nn.Linear(d_model, num_heads*d_head, bias=True)\n",
    "      self.W_K = torch.nn.Linear(d_model, num_heads*d_head, bias=True)\n",
    "      self.W_V = torch.nn.Linear(d_model, num_heads*d_head, bias=True)\n",
    "      self.W_O = torch.nn.Linear(num_heads*d_head, d_model, bias=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "      seq_len, batch_size, _ = x.shape\n",
    "\n",
    "      Q = self.W_Q(x).reshape(seq_len, batch_size, self.num_heads, self.d_head)\n",
    "      K = self.W_K(x).reshape(seq_len, batch_size, self.num_heads, self.d_head)\n",
    "      V = self.W_V(x).reshape(seq_len, batch_size, self.num_heads, self.d_head)\n",
    "\n",
    "      scaled_QK = torch.einsum(\"ibhd,jbhd->bhij\", Q, K) / math.sqrt(self.d_head)\n",
    "      # shape of scaled_QK is (batch_size, num_heads, seq_len, seq_len)\n",
    "      \n",
    "      #TODO\n",
    "      pos_k = seq_len // 2\n",
    "      for i in range(pos_k):\n",
    "          scaled_QK[:, :, i, pos_k:] = float('-inf')\n",
    "      for i in range(pos_k, seq_len):\n",
    "          scaled_QK[:, :, i, i+1:] = float('-inf')\n",
    "      #ENDTODO\n",
    "      \n",
    "      weights = F.softmax(scaled_QK, -1)\n",
    "      attention = torch.einsum(\"bhij,jbhd->ibhd\", weights, V)\n",
    "\n",
    "      result = self.W_O(attention.reshape(seq_len, batch_size,self.num_heads * self.d_head))\n",
    "\n",
    "      return result, weights"
   ],
   "metadata": {
    "id": "ENFiK_cTeDM0",
    "ExecuteTime": {
     "end_time": "2024-02-09T16:52:28.842709Z",
     "start_time": "2024-02-09T16:52:28.839153Z"
    }
   },
   "execution_count": 164,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Test your solution\n",
    "d_model = 8\n",
    "num_heads = 2\n",
    "d_head = 4\n",
    "k = 4\n",
    "batch_size = 2\n",
    "\n",
    "mha = MultiHeadAttention(d_model, num_heads, d_head)\n",
    "batched_x= torch.randn((2*k, batch_size, d_model))\n",
    "with torch.no_grad():\n",
    "  result, weights = mha(batched_x)\n",
    "print(\"Result:\", result)\n",
    "print(\"Weights:\", weights)"
   ],
   "metadata": {
    "id": "GDQ0a57NeB-z",
    "ExecuteTime": {
     "end_time": "2024-02-09T16:52:30.678864Z",
     "start_time": "2024-02-09T16:52:30.670714Z"
    }
   },
   "execution_count": 165,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result: tensor([[[ 0.6165,  0.2279, -0.2474, -0.3699, -0.3429,  0.3070, -0.2449,\n",
      "          -0.4021],\n",
      "         [ 0.3658,  0.2750, -0.2977, -0.0200,  0.1576, -0.0410, -0.2112,\n",
      "           0.0496]],\n",
      "\n",
      "        [[ 0.5868,  0.1048, -0.2370, -0.3896, -0.3943,  0.2168, -0.0643,\n",
      "          -0.2470],\n",
      "         [ 0.3489,  0.2874, -0.2876, -0.0153,  0.1794, -0.0389, -0.2040,\n",
      "           0.0562]],\n",
      "\n",
      "        [[ 0.6211,  0.2011, -0.2491, -0.3413, -0.3608,  0.2930, -0.2263,\n",
      "          -0.4227],\n",
      "         [ 0.3772,  0.2681, -0.3090, -0.0303,  0.1583, -0.0537, -0.2098,\n",
      "           0.0560]],\n",
      "\n",
      "        [[ 0.6297,  0.0413, -0.2484, -0.3042, -0.4244,  0.2260, -0.0458,\n",
      "          -0.4361],\n",
      "         [ 0.3354,  0.2986, -0.2832,  0.0012,  0.1969, -0.0436, -0.1920,\n",
      "           0.0652]],\n",
      "\n",
      "        [[ 0.6037,  0.2619, -0.2329, -0.3774, -0.3723,  0.2733, -0.3207,\n",
      "          -0.3864],\n",
      "         [ 0.2830,  0.3667, -0.2325, -0.0649,  0.2315,  0.0099, -0.1814,\n",
      "           0.0594]],\n",
      "\n",
      "        [[ 0.6096,  0.2347, -0.2510, -0.3323, -0.3959,  0.1955, -0.3384,\n",
      "          -0.3791],\n",
      "         [ 0.2807,  0.3236, -0.2551,  0.1057,  0.1412, -0.0864, -0.1772,\n",
      "           0.0679]],\n",
      "\n",
      "        [[ 0.5652,  0.2191, -0.2135, -0.2984, -0.4450,  0.1291, -0.2763,\n",
      "          -0.3208],\n",
      "         [ 0.3291,  0.4078, -0.2352, -0.0678,  0.0953,  0.0210, -0.2331,\n",
      "          -0.0152]],\n",
      "\n",
      "        [[ 0.5553,  0.1792, -0.1958, -0.3283, -0.4149,  0.1542, -0.1517,\n",
      "          -0.3140],\n",
      "         [ 0.2721,  0.3720, -0.2449,  0.0612,  0.1670, -0.0292, -0.1787,\n",
      "           0.0626]]])\n",
      "Weights: tensor([[[[0.3136, 0.1851, 0.2769, 0.2244, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.2098, 0.2542, 0.3404, 0.1956, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.2801, 0.2119, 0.2481, 0.2599, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.1478, 0.3574, 0.2264, 0.2684, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.2204, 0.1260, 0.2347, 0.2056, 0.2132, 0.0000, 0.0000, 0.0000],\n",
      "          [0.1745, 0.1269, 0.1810, 0.1814, 0.1737, 0.1625, 0.0000, 0.0000],\n",
      "          [0.0689, 0.1055, 0.1346, 0.1247, 0.1683, 0.2582, 0.1397, 0.0000],\n",
      "          [0.0708, 0.1396, 0.1143, 0.1056, 0.1414, 0.2118, 0.1377, 0.0788]],\n",
      "\n",
      "         [[0.2943, 0.0638, 0.4071, 0.2348, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.1842, 0.3443, 0.1712, 0.3003, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.2984, 0.0513, 0.4119, 0.2384, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.2570, 0.1517, 0.2981, 0.2932, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.2391, 0.0666, 0.2866, 0.2471, 0.1606, 0.0000, 0.0000, 0.0000],\n",
      "          [0.1955, 0.0832, 0.2154, 0.2110, 0.1499, 0.1450, 0.0000, 0.0000],\n",
      "          [0.1457, 0.1621, 0.1341, 0.1320, 0.1465, 0.1387, 0.1410, 0.0000],\n",
      "          [0.1063, 0.1536, 0.1085, 0.1318, 0.1060, 0.1052, 0.1483, 0.1403]]],\n",
      "\n",
      "\n",
      "        [[[0.2515, 0.2438, 0.2513, 0.2534, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.2087, 0.2458, 0.2824, 0.2631, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.2377, 0.2565, 0.2736, 0.2322, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.1842, 0.2438, 0.2751, 0.2969, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.1776, 0.2008, 0.2211, 0.1853, 0.2151, 0.0000, 0.0000, 0.0000],\n",
      "          [0.0999, 0.1499, 0.1785, 0.1839, 0.1446, 0.2431, 0.0000, 0.0000],\n",
      "          [0.1955, 0.1530, 0.1268, 0.1344, 0.1640, 0.0876, 0.1386, 0.0000],\n",
      "          [0.0925, 0.1132, 0.1338, 0.1262, 0.1081, 0.1955, 0.1234, 0.1074]],\n",
      "\n",
      "         [[0.2391, 0.2413, 0.2581, 0.2615, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.2297, 0.2549, 0.2405, 0.2749, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.2175, 0.2467, 0.2823, 0.2534, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.2144, 0.2535, 0.2373, 0.2949, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.1419, 0.1816, 0.2053, 0.2203, 0.2508, 0.0000, 0.0000, 0.0000],\n",
      "          [0.1668, 0.1700, 0.1700, 0.1658, 0.1664, 0.1611, 0.0000, 0.0000],\n",
      "          [0.1217, 0.1259, 0.1690, 0.1249, 0.2388, 0.0685, 0.1511, 0.0000],\n",
      "          [0.0931, 0.1214, 0.1099, 0.1486, 0.1026, 0.1757, 0.1288, 0.1200]]]])\n"
     ]
    }
   ]
  }
 ]
}
