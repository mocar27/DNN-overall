{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DxaMTckGDGgc"
      },
      "source": [
        "<center><img src='https://drive.google.com/uc?id=1_utx_ZGclmCwNttSe40kYA6VHzNocdET' height=\"60\"></center>\n",
        "\n",
        "AI TECH - Akademia Innowacyjnych Zastosowań Technologii Cyfrowych. Program Operacyjny Polska Cyfrowa na lata 2014-2020\n",
        "<hr>\n",
        "\n",
        "<center><img src='https://drive.google.com/uc?id=1BXZ0u3562N_MqCLcekI-Ens77Kk4LpPm'></center>\n",
        "\n",
        "<center>\n",
        "Projekt współfinansowany ze środków Unii Europejskiej w ramach Europejskiego Funduszu Rozwoju Regionalnego\n",
        "Program Operacyjny Polska Cyfrowa na lata 2014-2020,\n",
        "Oś Priorytetowa nr 3 \"Cyfrowe kompetencje społeczeństwa\" Działanie  nr 3.2 \"Innowacyjne rozwiązania na rzecz aktywizacji cyfrowej\"\n",
        "Tytuł projektu:  „Akademia Innowacyjnych Zastosowań Technologii Cyfrowych (AI Tech)”\n",
        "    </center>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6y4l5BmxTNNU"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import numpy as np\n",
        "from torchvision import datasets, transforms"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iHhqeGLsHcYl",
        "outputId": "eb96b9b4-9a33-4fa7-abbb-0a1b9f43ad8d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-11-06 22:43:04--  https://s3.amazonaws.com/img-datasets/mnist.npz\n",
            "Resolving s3.amazonaws.com (s3.amazonaws.com)... 52.216.44.184, 52.216.216.112, 52.216.138.61, ...\n",
            "Connecting to s3.amazonaws.com (s3.amazonaws.com)|52.216.44.184|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 11490434 (11M) [application/octet-stream]\n",
            "Saving to: ‘mnist.npz’\n",
            "\n",
            "mnist.npz           100%[===================>]  10.96M  37.0MB/s    in 0.3s    \n",
            "\n",
            "2023-11-06 22:43:04 (37.0 MB/s) - ‘mnist.npz’ saved [11490434/11490434]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget -O mnist.npz https://s3.amazonaws.com/img-datasets/mnist.npz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uutaqUkuVAuF"
      },
      "outputs": [],
      "source": [
        "# Let's read the mnist dataset\n",
        "\n",
        "def load_mnist(path='mnist.npz'):\n",
        "    with np.load(path) as f:\n",
        "        x_train, _y_train = f['x_train'], f['y_train']\n",
        "        x_test, _y_test = f['x_test'], f['y_test']\n",
        "\n",
        "    x_train = x_train.reshape(-1, 28 * 28) / 255.\n",
        "    x_test = x_test.reshape(-1, 28 * 28) / 255.\n",
        "\n",
        "    y_train = np.zeros((_y_train.shape[0], 10))\n",
        "    y_train[np.arange(_y_train.shape[0]), _y_train] = 1\n",
        "\n",
        "    y_test = np.zeros((_y_test.shape[0], 10))\n",
        "    y_test[np.arange(_y_test.shape[0]), _y_test] = 1\n",
        "\n",
        "    return (x_train, y_train), (x_test, y_test)\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = load_mnist()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T5PPE1ldTNNx"
      },
      "source": [
        "## Exercise 1\n",
        "\n",
        "In this exercise your task is to fill in the gaps in this code by implementing the backpropagation algorithm\n",
        "Once this is done, you can run the network on the MNIST example and see how it performs. Feel free to play with the parameters. Your model should achieve 90%+ accuracy after a few epochs.\n",
        "\n",
        "\n",
        "## Exercise 2 (Optional)\n",
        "\n",
        "Implement a \"fully vectorized\" version, i.e. one using matrix operations instead of going over examples one by one within a minibatch.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OsCgwvfHTNN0",
        "outputId": "0c9e0489-4975-4307-abe7-987ba8ed18a0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0, Accuracy: 0.7327\n",
            "Epoch: 1, Accuracy: 0.8513\n",
            "Epoch: 2, Accuracy: 0.881\n",
            "Epoch: 3, Accuracy: 0.8948\n",
            "Epoch: 4, Accuracy: 0.9047\n",
            "Epoch: 5, Accuracy: 0.9086\n",
            "Epoch: 6, Accuracy: 0.9138\n",
            "Epoch: 7, Accuracy: 0.9164\n",
            "Epoch: 8, Accuracy: 0.9193\n",
            "Epoch: 9, Accuracy: 0.9217\n",
            "Epoch: 10, Accuracy: 0.9234\n",
            "Epoch: 11, Accuracy: 0.924\n",
            "Epoch: 12, Accuracy: 0.9262\n",
            "Epoch: 13, Accuracy: 0.9276\n",
            "Epoch: 14, Accuracy: 0.9282\n",
            "Epoch: 15, Accuracy: 0.929\n",
            "Epoch: 16, Accuracy: 0.9299\n",
            "Epoch: 17, Accuracy: 0.9306\n",
            "Epoch: 18, Accuracy: 0.9321\n",
            "Epoch: 19, Accuracy: 0.9331\n",
            "Epoch: 20, Accuracy: 0.9339\n",
            "Epoch: 21, Accuracy: 0.935\n",
            "Epoch: 22, Accuracy: 0.9354\n",
            "Epoch: 23, Accuracy: 0.9359\n",
            "Epoch: 24, Accuracy: 0.9368\n",
            "Epoch: 25, Accuracy: 0.937\n",
            "Epoch: 26, Accuracy: 0.9379\n",
            "Epoch: 27, Accuracy: 0.9377\n",
            "Epoch: 28, Accuracy: 0.9378\n",
            "Epoch: 29, Accuracy: 0.9377\n",
            "Epoch: 30, Accuracy: 0.9379\n",
            "Epoch: 31, Accuracy: 0.9383\n",
            "Epoch: 32, Accuracy: 0.9379\n",
            "Epoch: 33, Accuracy: 0.9387\n",
            "Epoch: 34, Accuracy: 0.939\n",
            "Epoch: 35, Accuracy: 0.9393\n",
            "Epoch: 36, Accuracy: 0.9403\n",
            "Epoch: 37, Accuracy: 0.941\n",
            "Epoch: 38, Accuracy: 0.9414\n",
            "Epoch: 39, Accuracy: 0.9416\n",
            "Epoch: 40, Accuracy: 0.9415\n",
            "Epoch: 41, Accuracy: 0.9415\n",
            "Epoch: 42, Accuracy: 0.9418\n",
            "Epoch: 43, Accuracy: 0.9421\n",
            "Epoch: 44, Accuracy: 0.9421\n",
            "Epoch: 45, Accuracy: 0.9427\n",
            "Epoch: 46, Accuracy: 0.9425\n",
            "Epoch: 47, Accuracy: 0.9429\n",
            "Epoch: 48, Accuracy: 0.9428\n",
            "Epoch: 49, Accuracy: 0.9426\n"
          ]
        }
      ],
      "source": [
        "# Exercise 1\n",
        "\n",
        "def sigmoid(z):\n",
        "    return 1.0/(1.0+np.exp(-z))\n",
        "\n",
        "def sigmoid_prime(z):\n",
        "    # Derivative of the sigmoid\n",
        "    return sigmoid(z)*(1-sigmoid(z))\n",
        "\n",
        "class Network(object):\n",
        "    def __init__(self, sizes):\n",
        "        # initialize biases and weights with random normal distr.\n",
        "        # weights are indexed by target node first\n",
        "        self.num_layers = len(sizes)\n",
        "        self.sizes = sizes\n",
        "        self.biases = [np.random.randn(y, 1) for y in sizes[1:]]\n",
        "        self.weights = [np.random.randn(y, x) for x, y in zip(sizes[:-1], sizes[1:])]\n",
        "    def feedforward(self, a):\n",
        "        # Run the network on a single case\n",
        "        for b, w in zip(self.biases, self.weights):\n",
        "            a = sigmoid(np.dot(w, a)+b)\n",
        "        return a\n",
        "\n",
        "    def update_mini_batch(self, x_mini_batch, y_mini_batch, eta):\n",
        "        # Update networks weights and biases by applying a single step\n",
        "        # of gradient descent using backpropagation to compute the gradient.\n",
        "        # The gradient is computed for a mini_batch.\n",
        "        # eta is the learning rate\n",
        "        nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
        "        nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
        "        for x, y in zip(x_mini_batch, y_mini_batch):\n",
        "            delta_nabla_b, delta_nabla_w = self.backprop(x.reshape(784,1), y.reshape(10,1))\n",
        "            nabla_b = [nb+dnb for nb, dnb in zip(nabla_b, delta_nabla_b)]\n",
        "            nabla_w = [nw+dnw for nw, dnw in zip(nabla_w, delta_nabla_w)]\n",
        "        self.weights = [w-(eta/len(x_mini_batch))*nw\n",
        "                        for w, nw in zip(self.weights, nabla_w)]\n",
        "        self.biases = [b-(eta/len(x_mini_batch))*nb\n",
        "                       for b, nb in zip(self.biases, nabla_b)]\n",
        "\n",
        "    def backprop(self, x, y):\n",
        "        # For a single input (x,y) return a tuple of lists.\n",
        "        # First initialize the list of gradient arrays\n",
        "        delta_nabla_b = [np.zeros(b.shape) for b in self.biases] # Gradients over biases\n",
        "        delta_nabla_w = [np.zeros(w.shape) for w in self.weights] # Gradients over weights\n",
        "\n",
        "        # Feedforward - store all activations and weighted inputs\n",
        "\n",
        "        # Values before activation function, layer by layer, shapes (L_1), ..., (10).\n",
        "        zs = [] # z_i is the z_{i-1} * w_i\n",
        "        # Values after activation function (including inputs to the first layer), shapes (784), (L_1), ..., (10).\n",
        "        activations = [] # a_i is z_i with applied activation function\n",
        "        activations.append(x) # we add our input matrix to activations\n",
        "        i = 1\n",
        "\n",
        "        # here we just add to empty array values that are initially in the neurons of neural network\n",
        "        for w, b, a in zip(self.weights, self.biases, activations):\n",
        "            z = np.dot(w, activations[i-1]) + b # since values on whole layer is just multiplying matrix of weights with matrix of zs and to each we add bias[it]\n",
        "            zs.append(z)\n",
        "            activations.append(sigmoid(z))\n",
        "            i = i + 1\n",
        "\n",
        "        # Now go backward from the final cost applying backpropagation\n",
        "        # It’s a process of fine-tuning the weights of a network based on\n",
        "        # the errors or loss obtained in the previous epoch (iteration).\n",
        "        dLdg = self.cost_derivative(activations[-1], y)\n",
        "        dLdfs = []\n",
        "\n",
        "        # We are going backwards, not counting the first \"layer of activations\", since it's just the input\n",
        "        for W, g in zip(reversed(self.weights), reversed(activations[1:])):\n",
        "\n",
        "          # Formula for dL over df from the lecture.\n",
        "          dLdf = np.multiply(dLdg, np.multiply(g, (1 - g)))\n",
        "\n",
        "          # We are adding dL / df to the front list, but remember we are going backwards in our Neural Network.\n",
        "          dLdfs.append(dLdf)\n",
        "\n",
        "          # Formula from dL over dg from the lecture.\n",
        "          dLdg = np.matmul(W.T, dLdf)\n",
        "\n",
        "        # We are updating our deltas for each weight using matrices that we calculated in backprop\n",
        "        delta_nabla_w = [dLdf @ g.T for dLdf, g in zip(reversed(dLdfs), activations[:-1])]\n",
        "        # delta_nabla_b = [np.sum(dLdf, axis=1).reshape(dLdf.shape[0], 1) for dLdf in reversed(dLdfs)]\n",
        "\n",
        "        return (delta_nabla_b, delta_nabla_w)\n",
        "\n",
        "\n",
        "    def evaluate(self, x_test_data, y_test_data):\n",
        "        # Count the number of correct answers for test_data\n",
        "        test_results = [(np.argmax(self.feedforward(x_test_data[i].reshape(784,1))), np.argmax(y_test_data[i]))\n",
        "                        for i in range(len(x_test_data))]\n",
        "        # return accuracy\n",
        "        return np.mean([int(x == y) for (x, y) in test_results])\n",
        "\n",
        "    def cost_derivative(self, output_activations, y):\n",
        "        return (output_activations-y)\n",
        "\n",
        "    def SGD(self, training_data, epochs, mini_batch_size, eta, test_data=None):\n",
        "        x_train, y_train = training_data\n",
        "        if test_d5ata:\n",
        "            x_test, y_test = test_data\n",
        "        for j in range(epochs):\n",
        "            for i in range(x_train.shape[0] // mini_batch_size):\n",
        "                x_mini_batch = x_train[i*mini_batch_size:(i*mini_batch_size + mini_batch_size)]\n",
        "                y_mini_batch = y_train[i*mini_batch_size:(i*mini_batch_size + mini_batch_size)]\n",
        "                self.update_mini_batch(x_mini_batch, y_mini_batch, eta)\n",
        "            if test_data:\n",
        "                print(\"Epoch: {0}, Accuracy: {1}\".format(j, self.evaluate(x_test, y_test)))\n",
        "            else:\n",
        "                print(\"Epoch: {0}\".format(j))\n",
        "\n",
        "\n",
        "network = Network([784,30,10])\n",
        "network.SGD((x_train, y_train), epochs=50, mini_batch_size=100, eta=3., test_data=(x_test, y_test))\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}