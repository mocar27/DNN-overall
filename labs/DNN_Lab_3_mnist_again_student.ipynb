{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.7"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<center><img src='https://drive.google.com/uc?id=1_utx_ZGclmCwNttSe40kYA6VHzNocdET' height=\"60\"></center>\n",
        "\n",
        "AI TECH - Akademia Innowacyjnych Zastosowań Technologii Cyfrowych. Program Operacyjny Polska Cyfrowa na lata 2014-2020\n",
        "<hr>\n",
        "\n",
        "<center><img src='https://drive.google.com/uc?id=1BXZ0u3562N_MqCLcekI-Ens77Kk4LpPm'></center>\n",
        "\n",
        "<center>\n",
        "Projekt współfinansowany ze środków Unii Europejskiej w ramach Europejskiego Funduszu Rozwoju Regionalnego\n",
        "Program Operacyjny Polska Cyfrowa na lata 2014-2020,\n",
        "Oś Priorytetowa nr 3 \"Cyfrowe kompetencje społeczeństwa\" Działanie  nr 3.2 \"Innowacyjne rozwiązania na rzecz aktywizacji cyfrowej\"\n",
        "Tytuł projektu:  „Akademia Innowacyjnych Zastosowań Technologii Cyfrowych (AI Tech)”\n",
        "    </center>"
      ],
      "metadata": {
        "id": "84VetyCaGLyR"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ziZ9i7tXbO1T"
      },
      "source": [
        "In this lab, you will implement some of the techniques discussed in the lecture.\n",
        "\n",
        "Below you are given a solution to the previous scenario. Note that it has two serious drawbacks:\n",
        " * The output predictions do not sum up to one (i.e. it does not return a distribution) even though the images always contain exactly one digit.\n",
        " * It uses MSE coupled with output sigmoid which can lead to saturation and slow convergence\n",
        "\n",
        "**Task 1.** Use softmax instead of coordinate-wise sigmoid and use log-loss instead of MSE. Test to see if this improves convergence. Hint: When implementing backprop it might be easier to consider these two function as a single block and not even compute the gradient over the softmax values.\n",
        "\n",
        "**Task 2.** Implement L2 regularization and add momentum to the SGD algorithm. Play with different amounts of regularization and momentum. See if this improves accuracy/convergence.\n",
        "\n",
        "**Task 3 (optional).** Implement Adagrad, dropout and some simple data augmentations (e.g. tiny rotations/shifts etc.). Again, test to see how these changes improve accuracy/convergence.\n",
        "\n",
        "**Task 4.** Try adding extra layers to the network. Again, test how the changes you introduced affect accuracy/convergence. As a start, you can try this architecture: [784,100,30,10]\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P22HqX9AbO1a"
      },
      "source": [
        "import random\n",
        "import numpy as np\n",
        "from torchvision import datasets, transforms"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N9jGPaZhbO2B",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7e28f59c-4109-41cf-a1b2-c52179274fbd"
      },
      "source": [
        "# Let's read the mnist dataset\n",
        "\n",
        "def load_mnist(path='.'):\n",
        "    train_set = datasets.MNIST(path, train=True, download=True)\n",
        "    x_train = train_set.data.numpy()\n",
        "    _y_train = train_set.targets.numpy()\n",
        "\n",
        "    test_set = datasets.MNIST(path, train=False, download=True)\n",
        "    x_test = test_set.data.numpy()\n",
        "    _y_test = test_set.targets.numpy()\n",
        "\n",
        "    x_train = x_train.reshape((x_train.shape[0],28*28)) / 255.\n",
        "    x_test = x_test.reshape((x_test.shape[0],28*28)) / 255.\n",
        "\n",
        "    y_train = np.zeros((_y_train.shape[0], 10))\n",
        "    y_train[np.arange(_y_train.shape[0]), _y_train] = 1\n",
        "\n",
        "    y_test = np.zeros((_y_test.shape[0], 10))\n",
        "    y_test[np.arange(_y_test.shape[0]), _y_test] = 1\n",
        "\n",
        "    return (x_train, y_train), (x_test, y_test)\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = load_mnist()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9912422/9912422 [00:00<00:00, 22363907.26it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./MNIST/raw/train-images-idx3-ubyte.gz to ./MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 28881/28881 [00:00<00:00, 68788014.66it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./MNIST/raw/train-labels-idx1-ubyte.gz to ./MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "100%|██████████| 1648877/1648877 [00:00<00:00, 25663744.00it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./MNIST/raw/t10k-images-idx3-ubyte.gz to ./MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4542/4542 [00:00<00:00, 11353116.07it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./MNIST/raw/t10k-labels-idx1-ubyte.gz to ./MNIST/raw\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w3gAyqw4bO1p"
      },
      "source": [
        "def sigmoid(z):\n",
        "    return 1.0/(1.0+np.exp(-z))\n",
        "\n",
        "def sigmoid_prime(z):\n",
        "    # Derivative of the sigmoid\n",
        "    return sigmoid(z)*(1-sigmoid(z))\n",
        "\n",
        "def softmax(z):\n",
        "    assert len(z.shape) == 2\n",
        "\n",
        "    z_max = np.max(z, axis=1)\n",
        "    z_max = z_max[:, np.newaxis]\n",
        "    e_z = np.exp(z - z_max)\n",
        "\n",
        "    div = np.sum(e_z, axis=1)\n",
        "    div = div[:, np.newaxis]\n",
        "\n",
        "    return e_z / div\n",
        "\n",
        "def softmax_prime(z):\n",
        "    return softmax(z) * (1-softmax(z))\n",
        "\n",
        "def relu(z):\n",
        "  return np.max(0, z)\n",
        "\n",
        "def cross_entropy_loss_deriv(ps, ys):\n",
        "  return ((ps - ys) / (ps * (1 - ps)))"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FgEA2XRRbO2X",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a42d49a9-5d22-4bdd-911e-46ece927694d"
      },
      "source": [
        "class Network(object):\n",
        "    def __init__(self, sizes):\n",
        "        # initialize biases and weights with random normal distr.\n",
        "        # weights are indexed by target node first\n",
        "        self.num_layers = len(sizes)\n",
        "        self.sizes = sizes\n",
        "        self.biases = [np.random.randn(y, 1) for y in sizes[1:]]\n",
        "        self.weights = [np.random.randn(y, x) for x, y in zip(sizes[:-1], sizes[1:])]\n",
        "        self.momentum_w = [np.zeros_like(w) for w in self.weights]\n",
        "        self.momentum_b = [np.zeros_like(b) for b in self.biases]\n",
        "\n",
        "    def feedforward(self, a):\n",
        "        # Run the network on a batch\n",
        "        a = a.T\n",
        "        for b, w in zip(self.biases, self.weights):\n",
        "            a = sigmoid(np.matmul(w, a)+b)\n",
        "        return softmax(a) # Usage of softmax instead of sigmoid\n",
        "\n",
        "    def update_mini_batch(self, mini_batch, eta, lambd, gamma):\n",
        "        # Update networks weights and biases by applying a single step\n",
        "        # of gradient descent using backpropagation to compute the gradient.\n",
        "        # The gradient is computed for a mini_batch which is as in tensorflow API.\n",
        "        # eta is the learning rate\n",
        "        # gamma is used for momentum calculation\n",
        "        nabla_b, nabla_w = self.backprop(mini_batch[0].T,mini_batch[1].T)\n",
        "\n",
        "        self.momentum_w = [gamma * mw - (eta/len(mini_batch[0])) * nw - lambd * w\n",
        "                        for w, nw, mw in zip(self.weights, nabla_w, self.momentum_w)]\n",
        "        self.momentum_b = [gamma * mb - (eta/len(mini_batch[0])) * nb\n",
        "                       for b, nb, mb in zip(self.biases, nabla_b, self.momentum_b)]\n",
        "\n",
        "        self.weights = [w + mw\n",
        "                        for w, mw in zip(self.weights, self.momentum_w)]\n",
        "        self.biases = [b + mb\n",
        "                       for b, mb in zip(self.biases, self.momentum_b)]\n",
        "\n",
        "    def backprop(self, x, y):\n",
        "        # For a single input (x,y) return a pair of lists.\n",
        "        # First contains gradients over biases, second over weights.\n",
        "        g = x\n",
        "        gs = [g] # list to store all the gs, layer by layer\n",
        "        fs = [] # list to store all the fs, layer by layer\n",
        "        for b, w in zip(self.biases, self.weights):\n",
        "            f = np.dot(w, g)+b\n",
        "            fs.append(f)\n",
        "            g = sigmoid(f)\n",
        "            gs.append(g)\n",
        "        # backward pass <- both steps at once\n",
        "        dLdg = self.cost_derivative(gs[-1], y)\n",
        "        dLdfs = []\n",
        "        for w,g in reversed(list(zip(self.weights,gs[1:]))):\n",
        "            dLdf = np.multiply(dLdg,np.multiply(g,1-g))\n",
        "            dLdfs.append(dLdf)\n",
        "            dLdg = np.matmul(w.T, dLdf)\n",
        "\n",
        "        dLdWs = [np.matmul(dLdf,g.T) for dLdf,g in zip(reversed(dLdfs),gs[:-1])]\n",
        "        dLdBs = [np.sum(dLdf,axis=1).reshape(dLdf.shape[0],1) for dLdf in reversed(dLdfs)]\n",
        "\n",
        "        # We want to up\n",
        "        return (dLdBs,dLdWs)\n",
        "\n",
        "    def evaluate(self, test_data):\n",
        "        # Count the number of correct answers for test_data\n",
        "        pred = np.argmax(self.feedforward(test_data[0]),axis=0)\n",
        "\n",
        "        corr = np.argmax(test_data[1],axis=1).T\n",
        "        return np.mean(pred==corr)\n",
        "\n",
        "    def cost_derivative(self, output_activations, y):\n",
        "        # Usage of Log-loss loss function derivative\n",
        "        softmax = np.exp(output_activations)\n",
        "        softmax = softmax / softmax.sum(axis = 0)\n",
        "        return softmax - y\n",
        "\n",
        "    def SGD(self, training_data, epochs, mini_batch_size, eta, lambd=0.0, gamma=0.0, test_data=None):\n",
        "        x_train, y_train = training_data\n",
        "        if test_data:\n",
        "            x_test, y_test = test_data\n",
        "        for j in range(epochs):\n",
        "            for i in range(x_train.shape[0] // mini_batch_size):\n",
        "                x_mini_batch = x_train[(mini_batch_size*i):(mini_batch_size*(i+1))]\n",
        "                y_mini_batch = y_train[(mini_batch_size*i):(mini_batch_size*(i+1))]\n",
        "                self.update_mini_batch((x_mini_batch, y_mini_batch), eta, lambd=lambd, gamma=gamma)\n",
        "            if test_data:\n",
        "                print(\"Epoch: {0}, Accuracy: {1}\".format(j, self.evaluate((x_test, y_test))))\n",
        "            else:\n",
        "                print(\"Epoch: {0}\".format(j))\n",
        "\n",
        "\n",
        "network = Network([784,30,10])\n",
        "network.SGD((x_train, y_train), epochs=100, mini_batch_size=100, eta=3.0, lambd=0.000003, gamma = 0.95, test_data=(x_test, y_test))\n",
        "\n"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0, Accuracy: 0.9062\n",
            "Epoch: 1, Accuracy: 0.9126\n",
            "Epoch: 2, Accuracy: 0.9176\n",
            "Epoch: 3, Accuracy: 0.9186\n",
            "Epoch: 4, Accuracy: 0.9306\n",
            "Epoch: 5, Accuracy: 0.9338\n",
            "Epoch: 6, Accuracy: 0.9343\n",
            "Epoch: 7, Accuracy: 0.935\n",
            "Epoch: 8, Accuracy: 0.9344\n",
            "Epoch: 9, Accuracy: 0.94\n",
            "Epoch: 10, Accuracy: 0.9396\n",
            "Epoch: 11, Accuracy: 0.9375\n",
            "Epoch: 12, Accuracy: 0.9391\n",
            "Epoch: 13, Accuracy: 0.9387\n",
            "Epoch: 14, Accuracy: 0.9427\n",
            "Epoch: 15, Accuracy: 0.939\n",
            "Epoch: 16, Accuracy: 0.943\n",
            "Epoch: 17, Accuracy: 0.9423\n",
            "Epoch: 18, Accuracy: 0.9416\n",
            "Epoch: 19, Accuracy: 0.9385\n",
            "Epoch: 20, Accuracy: 0.9392\n",
            "Epoch: 21, Accuracy: 0.9401\n",
            "Epoch: 22, Accuracy: 0.9487\n",
            "Epoch: 23, Accuracy: 0.9395\n",
            "Epoch: 24, Accuracy: 0.9403\n",
            "Epoch: 25, Accuracy: 0.9442\n",
            "Epoch: 26, Accuracy: 0.9437\n",
            "Epoch: 27, Accuracy: 0.9482\n",
            "Epoch: 28, Accuracy: 0.9428\n",
            "Epoch: 29, Accuracy: 0.9504\n",
            "Epoch: 30, Accuracy: 0.9454\n",
            "Epoch: 31, Accuracy: 0.9454\n",
            "Epoch: 32, Accuracy: 0.9446\n",
            "Epoch: 33, Accuracy: 0.9441\n",
            "Epoch: 34, Accuracy: 0.9464\n",
            "Epoch: 35, Accuracy: 0.9488\n",
            "Epoch: 36, Accuracy: 0.9438\n",
            "Epoch: 37, Accuracy: 0.9492\n",
            "Epoch: 38, Accuracy: 0.9474\n",
            "Epoch: 39, Accuracy: 0.9494\n",
            "Epoch: 40, Accuracy: 0.9495\n",
            "Epoch: 41, Accuracy: 0.9454\n",
            "Epoch: 42, Accuracy: 0.9405\n",
            "Epoch: 43, Accuracy: 0.9445\n",
            "Epoch: 44, Accuracy: 0.9477\n",
            "Epoch: 45, Accuracy: 0.9507\n",
            "Epoch: 46, Accuracy: 0.9483\n",
            "Epoch: 47, Accuracy: 0.9481\n",
            "Epoch: 48, Accuracy: 0.9525\n",
            "Epoch: 49, Accuracy: 0.946\n",
            "Epoch: 50, Accuracy: 0.9474\n",
            "Epoch: 51, Accuracy: 0.9446\n",
            "Epoch: 52, Accuracy: 0.9501\n",
            "Epoch: 53, Accuracy: 0.9479\n",
            "Epoch: 54, Accuracy: 0.9539\n",
            "Epoch: 55, Accuracy: 0.954\n",
            "Epoch: 56, Accuracy: 0.947\n",
            "Epoch: 57, Accuracy: 0.9487\n",
            "Epoch: 58, Accuracy: 0.9524\n",
            "Epoch: 59, Accuracy: 0.9535\n",
            "Epoch: 60, Accuracy: 0.9532\n",
            "Epoch: 61, Accuracy: 0.9518\n",
            "Epoch: 62, Accuracy: 0.9463\n",
            "Epoch: 63, Accuracy: 0.9476\n",
            "Epoch: 64, Accuracy: 0.9465\n",
            "Epoch: 65, Accuracy: 0.9523\n",
            "Epoch: 66, Accuracy: 0.9487\n",
            "Epoch: 67, Accuracy: 0.9511\n",
            "Epoch: 68, Accuracy: 0.948\n",
            "Epoch: 69, Accuracy: 0.9479\n",
            "Epoch: 70, Accuracy: 0.9506\n",
            "Epoch: 71, Accuracy: 0.9492\n",
            "Epoch: 72, Accuracy: 0.9527\n",
            "Epoch: 73, Accuracy: 0.9441\n",
            "Epoch: 74, Accuracy: 0.9499\n",
            "Epoch: 75, Accuracy: 0.952\n",
            "Epoch: 76, Accuracy: 0.9521\n",
            "Epoch: 77, Accuracy: 0.9464\n",
            "Epoch: 78, Accuracy: 0.9505\n",
            "Epoch: 79, Accuracy: 0.947\n",
            "Epoch: 80, Accuracy: 0.9534\n",
            "Epoch: 81, Accuracy: 0.9497\n",
            "Epoch: 82, Accuracy: 0.9486\n",
            "Epoch: 83, Accuracy: 0.953\n",
            "Epoch: 84, Accuracy: 0.9534\n",
            "Epoch: 85, Accuracy: 0.9513\n",
            "Epoch: 86, Accuracy: 0.9503\n",
            "Epoch: 87, Accuracy: 0.9534\n",
            "Epoch: 88, Accuracy: 0.9516\n",
            "Epoch: 89, Accuracy: 0.9504\n",
            "Epoch: 90, Accuracy: 0.9512\n",
            "Epoch: 91, Accuracy: 0.9453\n",
            "Epoch: 92, Accuracy: 0.9526\n",
            "Epoch: 93, Accuracy: 0.9514\n",
            "Epoch: 94, Accuracy: 0.9472\n",
            "Epoch: 95, Accuracy: 0.9531\n",
            "Epoch: 96, Accuracy: 0.9507\n",
            "Epoch: 97, Accuracy: 0.9507\n",
            "Epoch: 98, Accuracy: 0.9501\n",
            "Epoch: 99, Accuracy: 0.952\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BKBj9u_cX6-O"
      },
      "source": [
        "# L2-reg:\n",
        "# C = loss + lambda(W^2) * sum() - suma wag^2\n",
        "# W = W - dC - lambda(W)\n",
        "# to jest nakladanie kary dodatkowej na algorytm\n",
        "# dC = dL * lambda(W)\n",
        "\n",
        "\n",
        "# Momentum:\n",
        "# jak mamy gradient dla danej wagi, to chcemy trzymac sume tych gradientow dla tej wagi\n",
        "# tylko zeby one byly podniesione tam do konkretnej potegi\n",
        "# tylko to srednia geometryczna, czyli kolejny moment to gradient ktory chcemy + gamma * moment, gamma zwykle duza ~ 0.95\n",
        "\n",
        "\n",
        "# w adegradzie uwazac zeby do niego nie dodawac tej czesci gradientu ktory wychodzi przy regularyzacji\n",
        "# maja byc to zwykle gradienty a nie te policzone po regularyzacji"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}